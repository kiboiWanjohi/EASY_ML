{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Session 3\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "P4ey80jOHYUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example a classification with MNIST dataset"
      ],
      "metadata": {
        "id": "TP_EOYILA2Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Create a simple CNN model\n",
        "\n",
        "Create a CNN for training grayscale image of size 28 x 28 pixels, with :\n",
        "\n",
        "\n",
        "*   One 2D convolutionnal layer with 16 filters\n",
        "*   One 2D convolutionnal layer with 32 filters\n",
        "*   One MaxPooling layer with stride=2 and kernel=2\n",
        "*   One Dense layer of 128 neurons\n",
        "*   One Dense layer of 64 neurons\n",
        "*   One ouput layer with 10 output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CEUbHT8DAyE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # Import the numpy library for numerical operations\n",
        "\n",
        "import torch # Import the PyTorch library\n",
        "from torch import nn # Import the neural network module from PyTorch\n",
        "\n",
        "# Import torchvision\n",
        "import torchvision # Import the torchvision library, which provides access to datasets, models, and image transformations\n",
        "from torchvision import datasets # Import the datasets module from torchvision, which contains standard datasets like MNIST\n",
        "from torchvision.transforms import ToTensor, transforms # Import the ToTensor transform to convert PIL Images to PyTorch Tensors\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# Import matplotlib for visualization\n",
        "import matplotlib.pyplot as plt # Import the matplotlib.pyplot module for creating plots and visualizations"
      ],
      "metadata": {
        "id": "ti5nrd3ABOhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    '''\n",
        "    class SimpleCNN(nn.Module): This line defines a new class SimpleCNN that inherits\n",
        "    from nn.Module. In PyTorch, nn.Module is the base class for all neural network modules.\n",
        "    Any custom neural network or layer must subclass nn.Module. This provides\n",
        "    the basic functionality for tracking parameters, submodules, and moving\n",
        "    data through the network.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        This is the constructor method for the SimpleCNN class.\n",
        "        When you create an instance of SimpleCNN (e.g., model = SimpleCNN()),\n",
        "        this method is called. It's where you define all the layers (or sub-modules)\n",
        "        that your neural network will use.\n",
        "        '''\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        '''\n",
        "        This line calls the constructor of the parent class (nn.Module).\n",
        "        It's crucial for proper initialization of the PyTorch module system.\n",
        "        '''\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "        '''\n",
        "        nn.Conv2d: This creates a 2D convolutional layer. Convolutional layers are\n",
        "        fundamental to CNNs as they learn hierarchical features from spatial data like images.\n",
        "        They do this by applying a small filter (or kernel) across the input image\n",
        "\n",
        "        in_channels=1: Indicates the input is a single-channel image (like grayscale).\n",
        "        If it were an RGB image, in_channels would be 3. The input tensor for this layer\n",
        "        would have a shape (Batch_Size, 1, Height, Width).\n",
        "\n",
        "        out_channels=16: This layer will produce 16 different \"feature maps.\"\n",
        "        Each feature map is the result of a unique 3x3 filter sliding across the input.\n",
        "        The output tensor will have a shape (Batch_Size, 16, New_Height, New_Width).\n",
        "\n",
        "        The choice of 16 for out_channels is a design decision made by the person creating\n",
        "        the neural network, not a value mathematically derived from the MNIST dataset itself.\n",
        "        In summary, 16 is a hyperparameter chosen to provide sufficient learning capacity for\n",
        "        the initial feature extraction on MNIST images without being excessively complex or\n",
        "        computationally expensive. It's a common practice based on empirical success in similar\n",
        "        image classification tasks.\n",
        "\n",
        "        kernel_size=3: The filter (or kernel) that slides over the image is a\n",
        "        3x3 matrix of learnable weights. This small size allows it to detect local patterns.\n",
        "\n",
        "        padding=1: A padding of 1 pixel is added around the input image. This is\n",
        "        often used to ensure that the spatial dimensions (height and width) of the output feature\n",
        "        maps remain the same as the input when stride=1 and kernel_size is odd (like 3). Without\n",
        "        padding, the output dimensions would shrink with each convolution.\n",
        "\n",
        "        The output spatial dimension formula for nn.Conv2d is:\n",
        "        Output_Dim = floor((Input_Dim - Kernel_Size + 2 * Padding) / Stride) + 1\n",
        "        If Input_Dim = H, Kernel_Size = 3, Padding = 1, Stride = 1, then Output_Dim = floor((H - 3 + 2*1) / 1) + 1 = floor(H - 1) + 1 = H.\n",
        "        '''\n",
        "        self.relu1 = nn.ReLU()\n",
        "        '''\n",
        "        ReLU (Rectified Linear Unit) is an activation function. It introduces non-linearity into the network. Without non-linearities,\n",
        "        a deep network would just be a series of linear transformations, equivalent to a single linear transformation, limiting its\n",
        "        ability to learn complex patterns.\n",
        "        '''\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        '''\n",
        "        This is the second convolutional layer. It takes the 16 feature maps from conv1 as its input channels and learns 32 new filters to\n",
        "        produce 32 output feature maps. It extracts higher-level features based on the features detected by conv1.\n",
        "\n",
        "        self.conv2 acts as a feature extractor that builds upon the simpler features learned by self.conv1. It takes the 16 feature maps\n",
        "        from conv1 as its input and learns to combine them into 32 new, more complex feature maps, which are then passed on to subsequent\n",
        "        layers for further processing (in this case, pooling).\n",
        "        '''\n",
        "        # ReLU activation function after the second convolution.\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        '''\n",
        "        nn.MaxPool2d: This specifically creates a 2D max pooling operation. It slides a window (defined by kernel_size) over the input\n",
        "        feature map and, for each window, outputs the maximum value within that window.\n",
        "\n",
        "        kernel_size=2:This defines the size of the window (or filter) that slides over the feature map. In this case, it's a 2x2 window.\n",
        "        The pooling operation will consider every 2x2 block of pixels and select the largest value from that block.For a given 2x2 input\n",
        "        patch, only the maximum of the four values is retained.\n",
        "\n",
        "        stride=2:This defines how many pixels the pooling window moves at each step. A stride of 2 means that after processing one 2x2\n",
        "        block, the window shifts 2 pixels horizontally, and then 2 pixels vertically for the next row. Since the stride equals the kernel\n",
        "        size, the pooling windows are non-overlapping.\n",
        "\n",
        "        Because the stride is 2, and kernel_size is 2, the spatial dimensions (height and width) of the input feature map will be halved.\n",
        "\n",
        "        the tensor shape after self.pool will be [Batch_Size, 32, 14, 14]\n",
        "        '''\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 128)\n",
        "        '''\n",
        "        This line defines the first fully connected (FC) layer, also known as a dense layer. Fully connected layers are typically used at the end\n",
        "        of a CNN after the convolutional and pooling layers have extracted and summarized high-level features from the input image. Their purpose\n",
        "        is to perform classification (or regression) based on these extracted features.\n",
        "\n",
        "        nn.Linear:This creates a linear transformation, which is the core operation of a fully connected layer. Mathematically, it performs a matrix\n",
        "        multiplication of the input vector with a weight matrix, and then adds a bias vector.\n",
        "        '''\n",
        "\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        '''\n",
        "        self.fc2 takes the 128 abstract features from the first fully connected layer and transforms them into an even more compressed, 64-element feature vector.\n",
        "        This 64-element vector represents a refined summary of the input image's characteristics, which is then passed to the final output layer for making\n",
        "        the classification decision. It acts as an additional hidden layer that refines the features before the final classification step.\n",
        "\n",
        "        nn.Linear: As before, this signifies a standard fully connected (or dense) layer, performing a linear transformation on its input.\n",
        "\n",
        "        128 (Input Features):This layer expects 128 input features (neurons). This number directly corresponds to the out_features of the\n",
        "        preceding self.fc1 layer. It means that self.fc2 takes the 128 high-level features generated by self.fc1 as its input.\n",
        "        The input tensor to self.fc2 will have a shape [Batch_Size, 128].\n",
        "        64 (Output Features):This defines the number of output features (neurons) that this layer will produce. This is another hyperparameter\n",
        "        chosen by the model designer. This layer further reduces the dimensionality of the feature representation from 128 down to 64. This\n",
        "        compression can help in learning more robust and discriminative features, and also reduces the number of parameters for the final output layer.\n",
        "        The output tensor from self.fc2 will have a shape [Batch_Size, 64].\n",
        "        '''\n",
        "\n",
        "        # Output layer, a fully connected layer with 64 input features and 10 output features (for 10 classes in MNIST).\n",
        "        self.out = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Define the forward pass of the network.\n",
        "        # Input x is the image tensor.\n",
        "        x = self.conv1(x) # Apply the first convolution.\n",
        "        x = self.relu1(x) # Apply the first ReLU activation.\n",
        "        x = self.conv2(x) # Apply the second convolution.\n",
        "        x = self.relu2(x) # Apply the second ReLU activation.\n",
        "        x = self.pool(x) # Apply the Max pooling layer.  Size:: [Batch_Size, 32, 14, 14]\n",
        "        x = torch.flatten(x, 1)\n",
        "        '''\n",
        "        We previously determined that the output of the self.pool layer has a shape of [Batch_Size, 32, 14, 14].\n",
        "        torch.flatten(x, 1) (which you have in your forward method) takes this 3D tensor (excluding the batch dimension) and reshapes it into a 1D vector.\n",
        "        Therefore, the number of elements in this flattened vector for each image is 32 channels * 14 height * 14 width = 6272.\n",
        "        This 6272 is the exact number of input features (in_features) for this nn.Linear layer.\n",
        "        '''\n",
        "        x = self.fc1(x) # Apply the first fully connected layer.\n",
        "        x = self.fc2(x) # Apply the second fully connected layer.\n",
        "        return self.out(x) # Apply the output layer and return the final output (raw scores for each class).\n"
      ],
      "metadata": {
        "id": "GbZWIIpNoZb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torchviz\n",
        "#!pip install torchinfo\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UkKxh04Vg546",
        "outputId": "f92329c4-f822-47fd-9bb5-705211cde221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.11/dist-packages (0.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from torchviz) (2.6.0+cu124)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->torchviz) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "# Setup model and device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "#  Print detailed model summary\n",
        "print(summary(\n",
        "    model,\n",
        "    input_size=(1, 1, 28, 28),     # Batch size 1, 1 channel, 28x28 input\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
        "    col_width=28,\n",
        "    row_settings=[\"depth\", \"var_names\"],\n",
        "    device=device.type,\n",
        "    verbose=2\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI69Jh1YlAI2",
        "outputId": "62f2d583-44b7-473f-cf0f-790f2cfb3b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================================================================================================\n",
            "Layer (type (var_name):depth-idx)        Input Shape                  Output Shape                 Param #                      Kernel Shape                 Mult-Adds\n",
            "====================================================================================================================================================================================\n",
            "SimpleCNN (SimpleCNN)                    [1, 1, 28, 28]               [1, 10]                      --                           --                           --\n",
            "├─Conv2d (conv1): 1-1                    [1, 1, 28, 28]               [1, 16, 28, 28]              160                          [3, 3]                       125,440\n",
            "│    └─weight                                                                                      ├─144                        [1, 16, 3, 3]\n",
            "│    └─bias                                                                                        └─16                         [16]\n",
            "├─ReLU (relu1): 1-2                      [1, 16, 28, 28]              [1, 16, 28, 28]              --                           --                           --\n",
            "├─Conv2d (conv2): 1-3                    [1, 16, 28, 28]              [1, 32, 28, 28]              4,640                        [3, 3]                       3,637,760\n",
            "│    └─weight                                                                                      ├─4,608                      [16, 32, 3, 3]\n",
            "│    └─bias                                                                                        └─32                         [32]\n",
            "├─ReLU (relu2): 1-4                      [1, 32, 28, 28]              [1, 32, 28, 28]              --                           --                           --\n",
            "├─MaxPool2d (pool): 1-5                  [1, 32, 28, 28]              [1, 32, 14, 14]              --                           2                            --\n",
            "├─Linear (fc1): 1-6                      [1, 6272]                    [1, 128]                     802,944                      --                           802,944\n",
            "│    └─weight                                                                                      ├─802,816                    [6272, 128]\n",
            "│    └─bias                                                                                        └─128                        [128]\n",
            "├─Linear (fc2): 1-7                      [1, 128]                     [1, 64]                      8,256                        --                           8,256\n",
            "│    └─weight                                                                                      ├─8,192                      [128, 64]\n",
            "│    └─bias                                                                                        └─64                         [64]\n",
            "├─Linear (out): 1-8                      [1, 64]                      [1, 10]                      650                          --                           650\n",
            "│    └─weight                                                                                      ├─640                        [64, 10]\n",
            "│    └─bias                                                                                        └─10                         [10]\n",
            "====================================================================================================================================================================================\n",
            "Total params: 816,650\n",
            "Trainable params: 816,650\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 4.58\n",
            "====================================================================================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.30\n",
            "Params size (MB): 3.27\n",
            "Estimated Total Size (MB): 3.57\n",
            "====================================================================================================================================================================================\n",
            "====================================================================================================================================================================================\n",
            "Layer (type (var_name):depth-idx)        Input Shape                  Output Shape                 Param #                      Kernel Shape                 Mult-Adds\n",
            "====================================================================================================================================================================================\n",
            "SimpleCNN (SimpleCNN)                    [1, 1, 28, 28]               [1, 10]                      --                           --                           --\n",
            "├─Conv2d (conv1): 1-1                    [1, 1, 28, 28]               [1, 16, 28, 28]              160                          [3, 3]                       125,440\n",
            "│    └─weight                                                                                      ├─144                        [1, 16, 3, 3]\n",
            "│    └─bias                                                                                        └─16                         [16]\n",
            "├─ReLU (relu1): 1-2                      [1, 16, 28, 28]              [1, 16, 28, 28]              --                           --                           --\n",
            "├─Conv2d (conv2): 1-3                    [1, 16, 28, 28]              [1, 32, 28, 28]              4,640                        [3, 3]                       3,637,760\n",
            "│    └─weight                                                                                      ├─4,608                      [16, 32, 3, 3]\n",
            "│    └─bias                                                                                        └─32                         [32]\n",
            "├─ReLU (relu2): 1-4                      [1, 32, 28, 28]              [1, 32, 28, 28]              --                           --                           --\n",
            "├─MaxPool2d (pool): 1-5                  [1, 32, 28, 28]              [1, 32, 14, 14]              --                           2                            --\n",
            "├─Linear (fc1): 1-6                      [1, 6272]                    [1, 128]                     802,944                      --                           802,944\n",
            "│    └─weight                                                                                      ├─802,816                    [6272, 128]\n",
            "│    └─bias                                                                                        └─128                        [128]\n",
            "├─Linear (fc2): 1-7                      [1, 128]                     [1, 64]                      8,256                        --                           8,256\n",
            "│    └─weight                                                                                      ├─8,192                      [128, 64]\n",
            "│    └─bias                                                                                        └─64                         [64]\n",
            "├─Linear (out): 1-8                      [1, 64]                      [1, 10]                      650                          --                           650\n",
            "│    └─weight                                                                                      ├─640                        [64, 10]\n",
            "│    └─bias                                                                                        └─10                         [10]\n",
            "====================================================================================================================================================================================\n",
            "Total params: 816,650\n",
            "Trainable params: 816,650\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 4.58\n",
            "====================================================================================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.30\n",
            "Params size (MB): 3.27\n",
            "Estimated Total Size (MB): 3.57\n",
            "====================================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load existing datasets : MNIST"
      ],
      "metadata": {
        "id": "5wYhpoYWn8V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define transformation pipeline for images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()  # Converts PIL images to PyTorch tensors with values in [0.0, 1.0]\n",
        "])\n",
        "\n",
        "# 2. Load the MNIST training dataset\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",             # Folder where MNIST will be stored\n",
        "    train=True,              # Load the training split\n",
        "    download=True,           # Download if not already present\n",
        "    transform=transform,     # Apply image transformation (ToTensor)\n",
        "    target_transform=None    # No transformation on labels\n",
        ")\n",
        "\n",
        "# 3. Load the MNIST test dataset\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,             # Load the test split\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# 4. Class labels in MNIST are digits 0 through 9\n",
        "class_names = train_data.classes  # This returns: ['0', '1', ..., '9']\n",
        "print(\"MNIST class names:\", class_names)\n",
        "\n",
        "# 5. Inspect dataset size and tensor shape\n",
        "print(f\"Number of training samples: {len(train_data)}\")\n",
        "print(f\"Number of test samples: {len(test_data)}\")\n",
        "\n",
        "# 6. Access and inspect a single training example\n",
        "image_tensor, label = train_data[0]  # Returns a tuple: (image, label)\n",
        "print(f\"Single image shape: {image_tensor.shape}  --> Format: [Channels, Height, Width]\")\n",
        "print(f\"Label for this image: {label}\")\n",
        "\n",
        "# 7. Visualize the first 10 unique digits from 0 to 9\n",
        "fig, axs = plt.subplots(1, 10, figsize=(16, 2))\n",
        "fig.suptitle(\"First Occurrence of Each Digit (0–9)\", fontsize=14)\n",
        "\n",
        "seen_digits = set()\n",
        "shown = 0\n",
        "\n",
        "# Loop through the dataset and plot the first instance of each class\n",
        "for idx, (img, lbl) in enumerate(train_data):\n",
        "    if lbl not in seen_digits:\n",
        "        axs[shown].imshow(img.squeeze(), cmap=\"gray\")\n",
        "        axs[shown].set_title(f\"Label: {lbl}\")\n",
        "        axs[shown].axis(\"off\")\n",
        "        seen_digits.add(lbl)\n",
        "        shown += 1\n",
        "    if shown == 10:\n",
        "        break\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QkWXDo2un__U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "cfe2e161-29e5-440b-8053-4ffb2e949915"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST class names: ['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n",
            "Number of training samples: 60000\n",
            "Number of test samples: 10000\n",
            "Single image shape: torch.Size([1, 28, 28])  --> Format: [Channels, Height, Width]\n",
            "Label for this image: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x200 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select just a subset\n",
        "train_data = Subset(train_data, range(1000))\n",
        "test_data = Subset(test_data, range(250))"
      ],
      "metadata": {
        "id": "P16ZNCxTH1wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preparation\n",
        "\n",
        "The DataLoader helps load data into a model for training and for inference.\n",
        "\n",
        "It turns a large Dataset into a Python iterable of smaller chunks. These smaller chunks are called batches or mini-batches and can be set by the batch_size parameter.\n",
        "\n",
        "Because it's more computationally efficient.\n",
        "\n",
        "In an ideal world you could do the forward pass and backward pass across all of your data at once. But once you start using really large datasets, unless you've got infinite computing power, it's easier to break them up into batches. So it's more computationally efficient it's also gives your model more opportunities to improve.\n",
        "\n",
        "With mini-batches (small portions of the data), gradient descent is performed more often per epoch (once per mini-batch rather than once per epoch)."
      ],
      "metadata": {
        "id": "6eG8RfC4fjbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader # Import the DataLoader class from torch.utils.data\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 1024 # Define the number of samples per batch\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data, # Use the training dataset\n",
        "    batch_size=BATCH_SIZE, # Set the batch size for the training data\n",
        "    shuffle=True # Shuffle the data at the beginning of each epoch for better training\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data, # Use the test dataset\n",
        "    batch_size=BATCH_SIZE, # Set the batch size for the test data\n",
        "    shuffle=False # No need to shuffle test data for evaluation\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") # Print the DataLoader objects\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") # Print the number of batches in the training DataLoader\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") # Print the number of batches in the test DataLoader"
      ],
      "metadata": {
        "id": "Adbe0sGJBFQ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e43d1856-d67d-4a73-ce3a-46c4470b925f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x7bdfac6b7390>, <torch.utils.data.dataloader.DataLoader object at 0x7bdfb1abe210>)\n",
            "Length of train dataloader: 1 batches of 1024\n",
            "Length of test dataloader: 1 batches of 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check for GPU availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3X03YBSRFmDM",
        "outputId": "89bff1d8-1274-4362-afa3-c5f7ced9e6ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch\n",
        "images, labels = next(iter(train_dataloader)) # Get the next batch of images and labels from the training DataLoader\n",
        "\n",
        "print(images.shape) # Print the shape of the images tensor in the batch (Batch Size, Channels, Height, Width)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO7fqReZxWtH",
        "outputId": "4b2d9cc9-5390-411e-a940-f0047cf7d79d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model :\n",
        "model = SimpleCNN() # Create an instance of the SimpleCNN model defined earlier\n",
        "\n",
        "# Predictions\n",
        "with torch.inference_mode(): # Disable gradient calculation for inference\n",
        "    for data, labels in test_dataloader : # Iterate through batches in the test DataLoader\n",
        "        preds = model(data) # Pass the image data through the model to get predictions\n",
        "\n",
        "preds[0,:] # Print the predictions (logits) for the first image in the last batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3xPITuLzrka",
        "outputId": "935376d4-0532-4c1c-c7b0-a64f5a73c4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0639, -0.0029,  0.0459, -0.0932, -0.0105, -0.1311, -0.0503,  0.1015,\n",
              "         0.1477, -0.0870])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score # Import the accuracy_score function from scikit-learn\n",
        "loss_fn = nn.CrossEntropyLoss() # Define the loss function as Cross-Entropy Loss, commonly used for classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Define the optimizer as Adam, with a learning rate of 0.01. It will update the model's parameters.\n",
        "\n",
        "\n",
        "# Training and testing loop\n",
        "epochs = 10 # Define the number of training epochs\n",
        "for epoch in range(epochs): # Loop through each epoch\n",
        "    for X_train, y_train in train_dataloader : # Iterate through batches of training data\n",
        "        model.train() # Set the model to training mode (enables dropout, batch normalization, etc.)\n",
        "        outputs = model(X_train) # Perform a forward pass to get model outputs (predictions)\n",
        "        loss = loss_fn(outputs, y_train) # Calculate the loss between the model outputs and the true labels\n",
        "\n",
        "        optimizer.zero_grad() # Zero the gradients of the optimizer\n",
        "        loss.backward() # Perform backpropagation to calculate gradients\n",
        "        optimizer.step() # Update the model's parameters based on the calculated gradients\n",
        "\n",
        "    model.eval() # Set the model to evaluation mode (disables dropout, batch normalization, etc.)\n",
        "    all_preds = [] # Initialize an empty list to store all predictions\n",
        "    all_labels = [] # Initialize an empty list to store all true labels\n",
        "    with torch.inference_mode(): # Disable gradient calculation for inference\n",
        "        for X_test, y_test in test_dataloader: # Iterate through batches of test data\n",
        "            test_preds = model(X_test) # Perform a forward pass to get model predictions on test data\n",
        "            test_pred_labels = torch.argmax(test_preds, dim=1) # Get the predicted class label by finding the index of the maximum logit\n",
        "\n",
        "            all_preds.append(test_pred_labels.cpu()) # Append the predicted labels to the list (move to CPU if on GPU)\n",
        "            all_labels.append(y_test.cpu()) # Append the true labels to the list (move to CPU if on GPU)\n",
        "\n",
        "    # Concaténer toutes les prédictions et labels\n",
        "    all_preds = torch.cat(all_preds) # Concatenate all predicted labels into a single tensor\n",
        "    all_labels = torch.cat(all_labels) # Concatenate all true labels into a single tensor\n",
        "    acc = accuracy_score(all_labels, all_preds) # Calculate the accuracy score\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Test Accuracy: {acc:.4f}\") # Print the epoch number, training loss, and test accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGG1iENphMeT",
        "outputId": "2f3c9034-98af-4bfa-a856-adb8481a8236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 | Loss: 2.3040 | Test Accuracy: 0.1440\n",
            "Epoch 2/10 | Loss: 3.8473 | Test Accuracy: 0.1960\n",
            "Epoch 3/10 | Loss: 3.9307 | Test Accuracy: 0.0560\n",
            "Epoch 4/10 | Loss: 4.8407 | Test Accuracy: 0.3320\n",
            "Epoch 5/10 | Loss: 2.4490 | Test Accuracy: 0.5760\n",
            "Epoch 6/10 | Loss: 1.7328 | Test Accuracy: 0.5200\n",
            "Epoch 7/10 | Loss: 1.6868 | Test Accuracy: 0.4680\n",
            "Epoch 8/10 | Loss: 1.5257 | Test Accuracy: 0.5360\n",
            "Epoch 9/10 | Loss: 1.2527 | Test Accuracy: 0.6640\n",
            "Epoch 10/10 | Loss: 0.9870 | Test Accuracy: 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Set batch size\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "\n",
        "# Print detailed architecture summary with batch size\n",
        "print(summary(\n",
        "    model,\n",
        "    input_size=(BATCH_SIZE, 1, 28, 28),\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"],\n",
        "    col_width=25,\n",
        "    row_settings=[\"depth\", \"var_names\"],\n",
        "    device=device.type,\n",
        "    verbose=2\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CH-gYNdnbf0",
        "outputId": "79ca4a27-aca2-40ae-fb1f-9ce93bfe3bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====================================================================================================================================================================\n",
            "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
            "=====================================================================================================================================================================\n",
            "SimpleCNN (SimpleCNN)                    [1024, 1, 28, 28]         [1024, 10]                --                        --                        --\n",
            "├─Conv2d (conv1): 1-1                    [1024, 1, 28, 28]         [1024, 16, 28, 28]        160                       [3, 3]                    128,450,560\n",
            "│    └─weight                                                                                ├─144                     [1, 16, 3, 3]\n",
            "│    └─bias                                                                                  └─16                      [16]\n",
            "├─ReLU (relu1): 1-2                      [1024, 16, 28, 28]        [1024, 16, 28, 28]        --                        --                        --\n",
            "├─Conv2d (conv2): 1-3                    [1024, 16, 28, 28]        [1024, 32, 28, 28]        4,640                     [3, 3]                    3,725,066,240\n",
            "│    └─weight                                                                                ├─4,608                   [16, 32, 3, 3]\n",
            "│    └─bias                                                                                  └─32                      [32]\n",
            "├─ReLU (relu2): 1-4                      [1024, 32, 28, 28]        [1024, 32, 28, 28]        --                        --                        --\n",
            "├─MaxPool2d (pool): 1-5                  [1024, 32, 28, 28]        [1024, 32, 14, 14]        --                        2                         --\n",
            "├─Linear (fc1): 1-6                      [1024, 6272]              [1024, 128]               802,944                   --                        822,214,656\n",
            "│    └─weight                                                                                ├─802,816                 [6272, 128]\n",
            "│    └─bias                                                                                  └─128                     [128]\n",
            "├─Linear (fc2): 1-7                      [1024, 128]               [1024, 64]                8,256                     --                        8,454,144\n",
            "│    └─weight                                                                                ├─8,192                   [128, 64]\n",
            "│    └─bias                                                                                  └─64                      [64]\n",
            "├─Linear (out): 1-8                      [1024, 64]                [1024, 10]                650                       --                        665,600\n",
            "│    └─weight                                                                                ├─640                     [64, 10]\n",
            "│    └─bias                                                                                  └─10                      [10]\n",
            "=====================================================================================================================================================================\n",
            "Total params: 816,650\n",
            "Trainable params: 816,650\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 4.68\n",
            "=====================================================================================================================================================================\n",
            "Input size (MB): 3.21\n",
            "Forward/backward pass size (MB): 309.94\n",
            "Params size (MB): 3.27\n",
            "Estimated Total Size (MB): 316.41\n",
            "=====================================================================================================================================================================\n",
            "=====================================================================================================================================================================\n",
            "Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
            "=====================================================================================================================================================================\n",
            "SimpleCNN (SimpleCNN)                    [1024, 1, 28, 28]         [1024, 10]                --                        --                        --\n",
            "├─Conv2d (conv1): 1-1                    [1024, 1, 28, 28]         [1024, 16, 28, 28]        160                       [3, 3]                    128,450,560\n",
            "│    └─weight                                                                                ├─144                     [1, 16, 3, 3]\n",
            "│    └─bias                                                                                  └─16                      [16]\n",
            "├─ReLU (relu1): 1-2                      [1024, 16, 28, 28]        [1024, 16, 28, 28]        --                        --                        --\n",
            "├─Conv2d (conv2): 1-3                    [1024, 16, 28, 28]        [1024, 32, 28, 28]        4,640                     [3, 3]                    3,725,066,240\n",
            "│    └─weight                                                                                ├─4,608                   [16, 32, 3, 3]\n",
            "│    └─bias                                                                                  └─32                      [32]\n",
            "├─ReLU (relu2): 1-4                      [1024, 32, 28, 28]        [1024, 32, 28, 28]        --                        --                        --\n",
            "├─MaxPool2d (pool): 1-5                  [1024, 32, 28, 28]        [1024, 32, 14, 14]        --                        2                         --\n",
            "├─Linear (fc1): 1-6                      [1024, 6272]              [1024, 128]               802,944                   --                        822,214,656\n",
            "│    └─weight                                                                                ├─802,816                 [6272, 128]\n",
            "│    └─bias                                                                                  └─128                     [128]\n",
            "├─Linear (fc2): 1-7                      [1024, 128]               [1024, 64]                8,256                     --                        8,454,144\n",
            "│    └─weight                                                                                ├─8,192                   [128, 64]\n",
            "│    └─bias                                                                                  └─64                      [64]\n",
            "├─Linear (out): 1-8                      [1024, 64]                [1024, 10]                650                       --                        665,600\n",
            "│    └─weight                                                                                ├─640                     [64, 10]\n",
            "│    └─bias                                                                                  └─10                      [10]\n",
            "=====================================================================================================================================================================\n",
            "Total params: 816,650\n",
            "Trainable params: 816,650\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.GIGABYTES): 4.68\n",
            "=====================================================================================================================================================================\n",
            "Input size (MB): 3.21\n",
            "Forward/backward pass size (MB): 309.94\n",
            "Params size (MB): 3.27\n",
            "Estimated Total Size (MB): 316.41\n",
            "=====================================================================================================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU version: Please make sure you change your run time to GPU"
      ],
      "metadata": {
        "id": "2w-eS0WVdzkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchdf\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torchvision # Assuming you're using torchvision datasets, if not, adjust train_data/test_data loading\n",
        "\n",
        "# Define a simple CNN model (assuming you have this defined elsewhere)\n",
        "# If you don't have this, you'll need to define your SimpleCNN class\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 10) # Adjust input features based on your image size (e.g., for 28x28 MNIST)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = x.view(x.size(0), -1) # Flatten the tensor\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# 1. Check for GPU availability\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 2. Load your dataset (example with MNIST, replace with your actual data loading)\n",
        "# If your data is already in tensor format, skip this and just ensure it's moved to device later\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Setup the batch size hyperparameter\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "# Turn datasets into iterables (batches)\n",
        "train_dataloader = DataLoader(train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Let's check out what we've created\n",
        "print(f\"Dataloaders: {train_dataloader, test_dataloader}\")\n",
        "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
        "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
        "\n",
        "# Get one batch (and move to device for checking)\n",
        "images, labels = next(iter(train_dataloader))\n",
        "images, labels = images.to(device), labels.to(device) # Move to device\n",
        "\n",
        "print(images.shape)\n",
        "\n",
        "# Instantiate model and move it to the GPU\n",
        "model = SimpleCNN().to(device) # Move model to the selected device\n",
        "\n",
        "# Predictions (initial check)\n",
        "with torch.inference_mode():\n",
        "    for data, labels_val in test_dataloader : # Renamed labels to labels_val to avoid conflict\n",
        "        data = data.to(device) # Move data to device\n",
        "        preds = model(data)\n",
        "        break # Just get one batch for the check\n",
        "\n",
        "print(\"Initial predictions (logits) for the first image in a batch:\", preds[0,:])\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training and testing loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    for X_train, y_train in train_dataloader :\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device) # Move data to device\n",
        "        outputs = model(X_train)\n",
        "        loss = loss_fn(outputs, y_train)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.inference_mode():\n",
        "        for X_test, y_test in test_dataloader:\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device) # Move data to device\n",
        "            test_preds = model(X_test)\n",
        "            test_pred_labels = torch.argmax(test_preds, dim=1)\n",
        "\n",
        "            all_preds.append(test_pred_labels.cpu()) # Move back to CPU for sklearn's accuracy_score\n",
        "            all_labels.append(y_test.cpu()) # Move back to CPU\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss.item():.4f} | Test Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "iJ2gk7vVGUji",
        "outputId": "c90c4a31-4da2-4f4f-82b5-ec1256ae3f9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-3686734144.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;31m# Assuming you're using torchvision datasets, if not, adjust train_data/test_data loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchdf'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}